{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook on Computer vision Crowd Counting\n",
    "\n",
    "(Version 1.0)\n",
    "\n",
    "<img src=\"pictures/TDI_logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook gives a summary of the machine learning model we used in this tool.  \n",
    "\n",
    "<br>\n",
    "\n",
    "**Explain the ML project here with a detailed description...**\n",
    "\n",
    "> About the project: Add more here later...\n",
    "\n",
    "Crowd Counting is a common application of computer vision. There are constantly new algorithms that are being invented as research is still active in this area. Our business case, however, drives our model selection. I've built and compared three approaches and I'll include some introduction on each of them in the later part of the document.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "**Explain this notebook's results here...**\n",
    "\n",
    "\n",
    "> The model selection space:\n",
    "\n",
    "> * Cascade Classifier Algorithm (Cascade)\n",
    "> * Tensorflow based Fast-RCNN (RCNN)\n",
    "> * Pytorch-based Multi-Column Convolutional Neural Network (MCNN)\n",
    "\n",
    "\n",
    "> We discuss and compare these 3 models and include some code nippets. \n",
    "\n",
<<<<<<< HEAD
    "> * Cascade is common for object detection and it's fast. In our business case it's possible to run detection on restaurant's video frames almost in real time.\n",
    "> * Problem with Cascase is that it's based on frontal face detection so accuracy drops when customer do not show face in the camera.\n",
    "> * RCNN is taking 2000 region proposals to feed into a CNN so computational speed is slow. Fast RCNN resolves this by using a heat map. Faster RCNN is even faster by adding a separate network so can also almost achieve real-time.\n",
    "> * Problem with RCNN is that the approach is new (from 2016) and there aren't many transfer learning model to use. When I trained my own model I found that it suffers from occulation (the scenario when crowd is dense and people covers each other). RCNN performs poorly when person do not have full body in the picture. occlusion-aware R-CNN is an active research area.\n",
    "> * MCNN use separate columns to handle the problem with different head size it's also more flexible on the input image size (sort of a one-size-fits-all solution). So could be generalized easily if commercialized.\n",
    "> * MCNN taked too long to train. I used Google Collab to train on the cloud using Pytorch, as I do not have a CUDA enabled GPU on my machine. I did not observe any advantage in either speed or accuracy.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cascade approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Github\\menu\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Apr 30 00:28:47 2020\n",
    "Cascade is native in OpenCV (import as cv2). The trained classifer is saved as a XML file.\n",
    "@author: georgehan\n",
    "\"\"\"\n",
    "import os\n",
    "#os.chdir('/Users/georgehan/TDI/Capstone/Smart_Menu')\n",
    "#os.chdir('/Users/georgehan/GitHub/menu')\n",
    "print(os.getcwd())\n",
    "#os.chdir(os.getcwd())\n",
    "import cv2\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pictures/mcd_short.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 customers!\n",
      "Hyperparameters are scaleFactor, minNeighbors, and minSize.\n"
     ]
    }
   ],
   "source": [
    "imagePath = \"pictures/mcd_short.jpg\"\n",
    "trainedWeights = \"trained_customer.xml\"\n",
    "\n",
    "customerTrainedWeights =  cv2.CascadeClassifier(trainedWeights)\n",
    "\n",
    "# Read the image\n",
    "image = cv2.imread(imagePath)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# detect customer in pic\n",
    "\n",
    "customers = customerTrainedWeights.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.2, # controlls fine (smaller) vs coarse (larger) trade off, needs to > 1.0\n",
    "        minNeighbors=5, # used to combine overlapping small boxes into big one\n",
    "        minSize=(60, 40) # box size, distance between recoginized customers\n",
    ")\n",
    "\n",
    "print(\"Found {0} customers!\".format(len(customers)))\n",
    "print(\"Hyperparameters are scaleFactor, minNeighbors, and minSize.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image written to file-system :  True\n",
      "pictures/mcd_very_long.jpg\n"
     ]
    }
   ],
   "source": [
    "# Draw a rectangle around the customers\n",
    "for (x, y, w, h) in customers:\n",
    "    cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "# cv2.imshow(\"customers found\", image)\n",
    "status = cv2.imwrite('customers_detect.jpg', image)\n",
    "print (\"Image written to file-system : \",status)\n",
    "print(imagePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mcd_short_marked.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Cascade approach can read more at:** http://www.willberger.org/cascade-haar-explained/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster R-CNN approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-30b806ccc8a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# RCNN uses OpenCV and tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# RCNN uses OpenCV and tensorflow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import time\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the main class for counting customers\n",
    "class customer_Counter:\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.detection_graph = tf.Graph()\n",
    "        with self.detection_graph.as_default():\n",
    "            od_graph_def = tf.GraphDef()\n",
    "            with tf.gfile.GFile(self.path, 'rb') as fid:\n",
    "                serialized_graph = fid.read()\n",
    "                od_graph_def.ParseFromString(serialized_graph)\n",
    "                tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "        self.default_graph = self.detection_graph.as_default()\n",
    "        self.sess = tf.Session(graph=self.detection_graph)\n",
    "\n",
    "        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0') # Defining tensors for the graph\n",
    "        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0') # Each box denotes part of image with a person detected \n",
    "        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0') # Score represents the confidence for the detected person\n",
    "        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')\n",
    "\n",
    "    def detect(self, image):\n",
    "        image_np_expanded = np.expand_dims(image, axis=0)\n",
    "        (boxes, scores, classes, num) = self.sess.run(\n",
    "            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],\n",
    "            feed_dict={self.image_tensor: image_np_expanded}) # Using the model for detection\n",
    "\n",
    "        im_height, im_width,_ = image.shape\n",
    "        boxes_list = [None for i in range(boxes.shape[1])]\n",
    "        for i in range(boxes.shape[1]):\n",
    "            boxes_list[i] = (int(boxes[0,i,0] * im_height),\n",
    "                        int(boxes[0,i,1]*im_width),\n",
    "                        int(boxes[0,i,2] * im_height),\n",
    "                        int(boxes[0,i,3]*im_width))\n",
    "\n",
    "        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])\n",
    "\n",
    "    \n",
    "    def close(self):\n",
    "        self.sess.close()\n",
    "        self.default_graph.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-1b84e9e38e0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./data/utils/my_model.pb'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mcustomer_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcustomer_Counter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mthreshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.4\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mno\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-589921313fe8>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetection_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetection_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mod_graph_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphDef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model_path = './data/utils/my_model.pb'\n",
    "    customer_counter = customer_Counter(path=model_path)\n",
    "    threshold = 0.4\n",
    "    no=1\n",
    "    for n in pbar(glob.glob(\"./data/images/test/*.jpg\")):\n",
    "        count=0\n",
    "        img = cv2.imread(n)\n",
    "        img = cv2.resize(img, (640, 480))\n",
    "\n",
    "        boxes, scores, classes, num = customer_counter.detect(img)\n",
    "\n",
    "        for i in range(len(boxes)):\n",
    "            if classes[i] == 1 and scores[i] > threshold:\n",
    "                box = boxes[i]\n",
    "                cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,0),2)\n",
    "                count+=1\n",
    "        cv2.putText(img,'Count = '+str(count),(10,400),cv2.FONT_HERSHEY_SIMPLEX, 1.25,(255,255,0),2,cv2.LINE_AA)\n",
    "        cv2.imwrite(\"./results/result%04i_menu_1.jpg\" %no, img)\n",
    "        no+=1\n",
    "print(\"\\n\\t\\t\\tSuccessfully saved all results!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed, RCNN model fails misearbly as it cannot detect unless has a full body of a person in the image, as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"RCNN_result.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Faster R-CNN can read more at**: https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCNN-Pytorch approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this appraoch we need to use visdom, a computer vision module open sourced by Facebook.\n",
    "Need to run following to install dependencies:\n",
    "> pip install visdom\n",
    "<br>\n",
    "> python -m visdom.server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-481a92ad26d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvisdom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import visdom\n",
    "import random\n",
    "\n",
    "from mcnn_model import MCNN\n",
    "from my_dataloader import CrowdDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main training script is as following. Note that if you do not have CUDA enabled GPU, then you need to use: \n",
    "> device = torch.device(\"cude:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-99ad401115bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mvis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvisdom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVisdom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmcnn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    torch.backends.cudnn.enabled=False\n",
    "    vis=visdom.Visdom()\n",
    "    device=torch.device(\"cuda\")\n",
    "    mcnn=MCNN().to(device)\n",
    "    criterion=nn.MSELoss(size_average=False).to(device)\n",
    "    optimizer = torch.optim.SGD(mcnn.parameters(), lr=1e-6,\n",
    "                                momentum=0.95)\n",
    "    \n",
    "    img_root='D:\\\\workspaceMaZhenwei\\\\GithubProject\\\\MCNN-pytorch\\\\data\\\\Shanghai_part_A\\\\train_data\\\\images'\n",
    "    gt_dmap_root='D:\\\\workspaceMaZhenwei\\\\GithubProject\\\\MCNN-pytorch\\\\data\\\\Shanghai_part_A\\\\train_data\\\\ground_truth'\n",
    "    dataset=CrowdDataset(img_root,gt_dmap_root,4)\n",
    "    dataloader=torch.utils.data.DataLoader(dataset,batch_size=1,shuffle=True)\n",
    "\n",
    "    test_img_root='D:\\\\workspaceMaZhenwei\\\\GithubProject\\\\MCNN-pytorch\\\\data\\\\Shanghai_part_A\\\\test_data\\\\images'\n",
    "    test_gt_dmap_root='D:\\\\workspaceMaZhenwei\\\\GithubProject\\\\MCNN-pytorch\\\\data\\\\Shanghai_part_A\\\\test_data\\\\ground_truth'\n",
    "    test_dataset=CrowdDataset(test_img_root,test_gt_dmap_root,4)\n",
    "    test_dataloader=torch.utils.data.DataLoader(test_dataset,batch_size=1,shuffle=False)\n",
    "\n",
    "    #training phase\n",
    "    if not os.path.exists('./checkpoints'):\n",
    "        os.mkdir('./checkpoints')\n",
    "    min_mae=10000\n",
    "    min_epoch=0\n",
    "    train_loss_list=[]\n",
    "    epoch_list=[]\n",
    "    test_error_list=[]\n",
    "    for epoch in range(0,2000):\n",
    "\n",
    "        mcnn.train()\n",
    "        epoch_loss=0\n",
    "        for i,(img,gt_dmap) in enumerate(dataloader):\n",
    "            img=img.to(device)\n",
    "            gt_dmap=gt_dmap.to(device)\n",
    "            # forward propagation\n",
    "            et_dmap=mcnn(img)\n",
    "            # calculate loss\n",
    "            loss=criterion(et_dmap,gt_dmap)\n",
    "            epoch_loss+=loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        #print(\"epoch:\",epoch,\"loss:\",epoch_loss/len(dataloader))\n",
    "        epoch_list.append(epoch)\n",
    "        train_loss_list.append(epoch_loss/len(dataloader))\n",
    "        torch.save(mcnn.state_dict(),'./checkpoints/epoch_'+str(epoch)+\".param\")\n",
    "\n",
    "        mcnn.eval()\n",
    "        mae=0\n",
    "        for i,(img,gt_dmap) in enumerate(test_dataloader):\n",
    "            img=img.to(device)\n",
    "            gt_dmap=gt_dmap.to(device)\n",
    "            # forward propagation\n",
    "            et_dmap=mcnn(img)\n",
    "            mae+=abs(et_dmap.data.sum()-gt_dmap.data.sum()).item()\n",
    "            del img,gt_dmap,et_dmap\n",
    "        if mae/len(test_dataloader)<min_mae:\n",
    "            min_mae=mae/len(test_dataloader)\n",
    "            min_epoch=epoch\n",
    "        test_error_list.append(mae/len(test_dataloader))\n",
    "        print(\"epoch:\"+str(epoch)+\" error:\"+str(mae/len(test_dataloader))+\" min_mae:\"+str(min_mae)+\" min_epoch:\"+str(min_epoch))\n",
    "        vis.line(win=1,X=epoch_list, Y=train_loss_list, opts=dict(title='train_loss'))\n",
    "        vis.line(win=2,X=epoch_list, Y=test_error_list, opts=dict(title='test_error'))\n",
    "        # show an image\n",
    "        index=random.randint(0,len(test_dataloader)-1)\n",
    "        img,gt_dmap=test_dataset[index]\n",
    "        vis.image(win=3,img=img,opts=dict(title='img'))\n",
    "        vis.image(win=4,img=gt_dmap/(gt_dmap.max())*255,opts=dict(title='gt_dmap('+str(gt_dmap.sum())+')'))\n",
    "        img=img.unsqueeze(0).to(device)\n",
    "        gt_dmap=gt_dmap.unsqueeze(0)\n",
    "        et_dmap=mcnn(img)\n",
    "        et_dmap=et_dmap.squeeze(0).detach().cpu().numpy()\n",
    "        vis.image(win=5,img=et_dmap/(et_dmap.max())*255,opts=dict(title='et_dmap('+str(et_dmap.sum())+')'))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model implementation script is saved as Pytorch_MCNN.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can read more on the ideas of MCNN at:** https://xiaoyuliu.github.io/2017/05/24/image-crowd-counting-mcnn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, considering the restaurant needs real-time speed, with reasonable accuracy and close up cameras. We choose Cascade classifier as it makes the most business sense. We can overcome the draw back of face detection requirement by placing the detection camera always facing the customer queue.\n",
    "<br>\n",
    "<br>\n",
    "This model selection is based on business case scenario.\n",
    "\n",
    "<br>\n",
    "Can add more here... on possible extensions. Feasibiliy of the this tool etc...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
